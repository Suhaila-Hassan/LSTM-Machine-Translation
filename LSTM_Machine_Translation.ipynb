{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Suhaila-Hassan/LSTM-Machine-Translation/blob/main/LSTM_Machine_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Machine Translation (English-French)"
      ],
      "metadata": {
        "id": "KnnQ6oWQKvWQ"
      },
      "id": "KnnQ6oWQKvWQ"
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install --upgrade torch torchvision torchaudio\n",
        "#!pip install --upgrade datasets fsspec"
      ],
      "metadata": {
        "id": "tuActYTtQ-Z-"
      },
      "id": "tuActYTtQ-Z-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "nGxNj3w5Kcpc"
      },
      "id": "nGxNj3w5Kcpc"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from datasets import load_dataset\n",
        "from collections import Counter\n",
        "import re\n",
        "import random"
      ],
      "metadata": {
        "id": "2tMDQ_3qSohM"
      },
      "id": "2tMDQ_3qSohM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing"
      ],
      "metadata": {
        "id": "GQjC0M5dKtFa"
      },
      "id": "GQjC0M5dKtFa"
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    return re.findall(r\"\\b\\w+\\b\", text.lower())\n",
        "\n",
        "def build_vocab(sentences, max_size=5000):\n",
        "    counter = Counter()\n",
        "    for s in sentences:\n",
        "        counter.update(tokenize(s))\n",
        "    vocab = {\"<pad>\": 0, \"<unk>\": 1, \"<sos>\": 2, \"<eos>\": 3}\n",
        "    for word, _ in counter.most_common(max_size - len(vocab)):\n",
        "        vocab[word] = len(vocab)\n",
        "    return vocab\n",
        "\n",
        "def encode(sentence, vocab, max_len):\n",
        "    tokens = [\"<sos>\"] + tokenize(sentence) + [\"<eos>\"]\n",
        "    token_ids = [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
        "    return token_ids[:max_len] + [vocab[\"<pad>\"]] * max(0, max_len - len(token_ids))"
      ],
      "metadata": {
        "id": "TfimVnRZSr9r"
      },
      "id": "TfimVnRZSr9r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Translation Dataset Class"
      ],
      "metadata": {
        "id": "CEwJlP52LZeC"
      },
      "id": "CEwJlP52LZeC"
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, data, src_vocab, tgt_vocab, max_len=20):\n",
        "        self.pairs = [(ex['translation']['en'], ex['translation']['fr']) for ex in data]\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src, tgt = self.pairs[idx]\n",
        "        src_ids = encode(src, self.src_vocab, self.max_len)\n",
        "        tgt_ids = encode(tgt, self.tgt_vocab, self.max_len)\n",
        "        return torch.tensor(src_ids), torch.tensor(tgt_ids)"
      ],
      "metadata": {
        "id": "wcTY48lTSvfc"
      },
      "id": "wcTY48lTSvfc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Translation Model Class"
      ],
      "metadata": {
        "id": "S9S3zYTuK6PF"
      },
      "id": "S9S3zYTuK6PF"
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleLSTMTranslator(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim, emb_dim=128, hidden_dim=256, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.dropout(self.embedding(x))\n",
        "        outputs, _ = self.lstm(embedded)\n",
        "        logits = self.fc(outputs)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "PU-QbbG7Sy7a"
      },
      "id": "PU-QbbG7Sy7a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and Prepare Dataset"
      ],
      "metadata": {
        "id": "ksnOOJnlLR2h"
      },
      "id": "ksnOOJnlLR2h"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"opus_books\", \"en-fr\", split=\"train[:25%]\")\n",
        "english_sentences = [x['translation']['en'] for x in dataset]\n",
        "french_sentences = [x['translation']['fr'] for x in dataset]\n",
        "\n",
        "src_vocab = build_vocab(english_sentences)\n",
        "tgt_vocab = build_vocab(french_sentences)\n",
        "inv_tgt_vocab = {idx: word for word, idx in tgt_vocab.items()}\n",
        "\n",
        "train_dataset = TranslationDataset(dataset, src_vocab, tgt_vocab, max_len=20)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "186f-Gk7S3UJ"
      },
      "id": "186f-Gk7S3UJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop"
      ],
      "metadata": {
        "id": "-JrjwhNvK-L9"
      },
      "id": "-JrjwhNvK-L9"
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SimpleLSTMTranslator(input_dim=len(src_vocab), output_dim=len(tgt_vocab)).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=src_vocab[\"<pad>\"])\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "ApZDpAWIS7nF"
      },
      "id": "ApZDpAWIS7nF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(100):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for src_batch, tgt_batch in train_loader:\n",
        "        src_batch, tgt_batch = src_batch.to(device), tgt_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src_batch)\n",
        "        loss = criterion(output.view(-1, output.size(-1)), tgt_batch.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch + 1} Loss: {total_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKoLLkKcTBjD",
        "outputId": "5d83a732-8398-44b1-c427-70e7847074fb"
      },
      "id": "nKoLLkKcTBjD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Loss: 2738.7507\n",
            "Epoch 2 Loss: 2506.4531\n",
            "Epoch 3 Loss: 2413.4967\n",
            "Epoch 4 Loss: 2343.9474\n",
            "Epoch 5 Loss: 2286.5246\n",
            "Epoch 6 Loss: 2238.7454\n",
            "Epoch 7 Loss: 2197.2786\n",
            "Epoch 8 Loss: 2161.3330\n",
            "Epoch 9 Loss: 2128.2579\n",
            "Epoch 10 Loss: 2098.6260\n",
            "Epoch 11 Loss: 2072.2800\n",
            "Epoch 12 Loss: 2047.3917\n",
            "Epoch 13 Loss: 2025.7697\n",
            "Epoch 14 Loss: 2004.8856\n",
            "Epoch 15 Loss: 1985.0059\n",
            "Epoch 16 Loss: 1967.2796\n",
            "Epoch 17 Loss: 1950.0536\n",
            "Epoch 18 Loss: 1934.7092\n",
            "Epoch 19 Loss: 1919.2958\n",
            "Epoch 20 Loss: 1904.9107\n",
            "Epoch 21 Loss: 1892.2956\n",
            "Epoch 22 Loss: 1879.4934\n",
            "Epoch 23 Loss: 1867.2346\n",
            "Epoch 24 Loss: 1855.0782\n",
            "Epoch 25 Loss: 1844.7754\n",
            "Epoch 26 Loss: 1834.0038\n",
            "Epoch 27 Loss: 1823.8037\n",
            "Epoch 28 Loss: 1814.0546\n",
            "Epoch 29 Loss: 1804.8464\n",
            "Epoch 30 Loss: 1796.5131\n",
            "Epoch 31 Loss: 1787.5508\n",
            "Epoch 32 Loss: 1780.4621\n",
            "Epoch 33 Loss: 1771.7572\n",
            "Epoch 34 Loss: 1764.3378\n",
            "Epoch 35 Loss: 1756.9680\n",
            "Epoch 36 Loss: 1750.0656\n",
            "Epoch 37 Loss: 1742.8110\n",
            "Epoch 38 Loss: 1736.7917\n",
            "Epoch 39 Loss: 1730.8754\n",
            "Epoch 40 Loss: 1725.0245\n",
            "Epoch 41 Loss: 1718.7550\n",
            "Epoch 42 Loss: 1713.3333\n",
            "Epoch 43 Loss: 1707.3758\n",
            "Epoch 44 Loss: 1702.3021\n",
            "Epoch 45 Loss: 1696.9112\n",
            "Epoch 46 Loss: 1692.7248\n",
            "Epoch 47 Loss: 1686.8729\n",
            "Epoch 48 Loss: 1682.7273\n",
            "Epoch 49 Loss: 1677.7738\n",
            "Epoch 50 Loss: 1673.2250\n",
            "Epoch 51 Loss: 1669.3414\n",
            "Epoch 52 Loss: 1664.3038\n",
            "Epoch 53 Loss: 1661.2273\n",
            "Epoch 54 Loss: 1658.0900\n",
            "Epoch 55 Loss: 1653.7853\n",
            "Epoch 56 Loss: 1648.7682\n",
            "Epoch 57 Loss: 1645.9256\n",
            "Epoch 58 Loss: 1642.1277\n",
            "Epoch 59 Loss: 1638.2432\n",
            "Epoch 60 Loss: 1634.9039\n",
            "Epoch 61 Loss: 1630.7428\n",
            "Epoch 62 Loss: 1628.4855\n",
            "Epoch 63 Loss: 1624.2246\n",
            "Epoch 64 Loss: 1621.0685\n",
            "Epoch 65 Loss: 1619.5577\n",
            "Epoch 66 Loss: 1615.9040\n",
            "Epoch 67 Loss: 1612.2649\n",
            "Epoch 68 Loss: 1609.8473\n",
            "Epoch 69 Loss: 1606.5883\n",
            "Epoch 70 Loss: 1604.3024\n",
            "Epoch 71 Loss: 1600.1162\n",
            "Epoch 72 Loss: 1598.3064\n",
            "Epoch 73 Loss: 1595.3659\n",
            "Epoch 74 Loss: 1593.2157\n",
            "Epoch 75 Loss: 1589.6258\n",
            "Epoch 76 Loss: 1588.1854\n",
            "Epoch 77 Loss: 1585.7821\n",
            "Epoch 78 Loss: 1584.0088\n",
            "Epoch 79 Loss: 1579.5247\n",
            "Epoch 80 Loss: 1577.7530\n",
            "Epoch 81 Loss: 1575.4131\n",
            "Epoch 82 Loss: 1572.9127\n",
            "Epoch 83 Loss: 1570.6781\n",
            "Epoch 84 Loss: 1570.3583\n",
            "Epoch 85 Loss: 1567.0609\n",
            "Epoch 86 Loss: 1565.0405\n",
            "Epoch 87 Loss: 1562.2953\n",
            "Epoch 88 Loss: 1561.4005\n",
            "Epoch 89 Loss: 1558.5924\n",
            "Epoch 90 Loss: 1556.6562\n",
            "Epoch 91 Loss: 1555.4243\n",
            "Epoch 92 Loss: 1552.2493\n",
            "Epoch 93 Loss: 1551.0152\n",
            "Epoch 94 Loss: 1548.6717\n",
            "Epoch 95 Loss: 1546.1179\n",
            "Epoch 96 Loss: 1544.7529\n",
            "Epoch 97 Loss: 1543.4290\n",
            "Epoch 98 Loss: 1541.5625\n",
            "Epoch 99 Loss: 1540.2713\n",
            "Epoch 100 Loss: 1537.2035\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference/Translation Function"
      ],
      "metadata": {
        "id": "t4bhXbcfLLbh"
      },
      "id": "t4bhXbcfLLbh"
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(sentence, model, src_vocab, tgt_vocab, inv_tgt_vocab, max_len=20, device='cpu'):\n",
        "    model.eval()\n",
        "    input_ids = encode(sentence, src_vocab, max_len)\n",
        "    input_tensor = torch.tensor(input_ids).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        predicted_ids = output.argmax(dim=-1).squeeze(0).tolist()\n",
        "\n",
        "    tokens = []\n",
        "    for idx in predicted_ids:\n",
        "        word = inv_tgt_vocab.get(idx, \"<unk>\")\n",
        "        if word in {\"<pad>\", \"<eos>\"}:\n",
        "            break\n",
        "        tokens.append(word)\n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "0sP_cJPnTGK4"
      },
      "id": "0sP_cJPnTGK4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Translation Example"
      ],
      "metadata": {
        "id": "qfA4KkMOLGW0"
      },
      "id": "qfA4KkMOLGW0"
    },
    {
      "cell_type": "code",
      "source": [
        "english_input = \"I love books\"\n",
        "french_output = translate_sentence(english_input, model, src_vocab, tgt_vocab, inv_tgt_vocab, device=device)\n",
        "print(\"Translation Example\")\n",
        "print(f\"English: {english_input}\")\n",
        "print(f\"French (predicted): {french_output}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44221c9c-8bd9-4cff-ae16-213831003439",
        "id": "7DGU000kQi43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Translation Example\n",
            "English: I love books\n",
            "French (predicted): <sos> je aime ai livres\n"
          ]
        }
      ],
      "id": "7DGU000kQi43"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wsWS_60SVKQU"
      },
      "id": "wsWS_60SVKQU",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}